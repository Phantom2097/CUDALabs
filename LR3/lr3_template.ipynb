{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyPwinpGOqCAEcNID7pzdhhY"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Pty02KQS-Uw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732437283463,
     "user_tz": -180,
     "elapsed": 169128,
     "user": {
      "displayName": "Nikita Velichko",
      "userId": "03447069592701329521"
     }
    },
    "outputId": "e4991507-0c4d-4422-dafd-d759daa11d3f",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.203022Z",
     "start_time": "2024-12-02T16:50:57.199934Z"
    }
   },
   "source": "# !pip install pycuda",
   "outputs": [],
   "execution_count": 254
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "id": "RCGOP01_Tl1I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from __future__ import division\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from mpmath.libmp import normalize\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "from pycuda.elementwise import ElementwiseKernel\n",
    "import numpy as np\n",
    "from multiprocessing import Queue\n",
    "import csv\n",
    "from time import time\n",
    "\n",
    "MAX_ENTROPY = 1"
   ],
   "metadata": {
    "id": "jDh_Wq7uTKPH",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.224103Z",
     "start_time": "2024-12-02T16:50:57.220146Z"
    }
   },
   "outputs": [],
   "execution_count": 255
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "UVKNZNAMUZwx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732437555410,
     "user_tz": -180,
     "elapsed": 13122,
     "user": {
      "displayName": "Nikita Velichko",
      "userId": "03447069592701329521"
     }
    },
    "outputId": "a9d570d1-488d-4e16-9323-69e21a765f71",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.263052Z",
     "start_time": "2024-12-02T16:50:57.259794Z"
    }
   },
   "outputs": [],
   "execution_count": 256
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Метод кросс энтропии"
   ],
   "metadata": {
    "id": "Y-l49p0bTpwp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def cross_entropy(predictions=None, ground_truth=None):\n",
    "    if predictions is None or ground_truth is None:\n",
    "        raise Exception(\"Error!  Both predictions and ground truth must be float32 arrays\")\n",
    "    p = np.array(predictions).copy()\n",
    "    y = np.array(ground_truth).copy()\n",
    "    if p.shape != y.shape:\n",
    "        raise Exception(\"Error!  Both predictions and ground_truth must have same shape.\")\n",
    "    if len(p.shape) != 2:\n",
    "        raise Exception(\"Error!  Both predictions and ground_truth must be 2D arrays.\")\n",
    "    total_entropy = 0\n",
    "    \n",
    "    # epsilon = 1e-12  # Малое значение для избежания логарифма нуля\n",
    "    # p = np.clip(p, epsilon, 1 - epsilon)  # Ограничиваем значения в пределах [epsilon, 1-epsilon]\n",
    "    \n",
    "    for i in range(p.shape[0]):\n",
    "        for j in range(p.shape[1]):\n",
    "            if y[i,j] == 1:\n",
    "                total_entropy += min( np.abs( np.nan_to_num(  np.log( p[i,j] ) ) ) , MAX_ENTROPY)\n",
    "            else:\n",
    "                total_entropy += min( np.abs( np.nan_to_num( np.log( 1 - p[i,j] ) ) ), MAX_ENTROPY)\n",
    "    return total_entropy / p.size"
   ],
   "metadata": {
    "id": "08UKP0tJTLI5",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.273418Z",
     "start_time": "2024-12-02T16:50:57.269067Z"
    }
   },
   "outputs": [],
   "execution_count": 257
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Реализация плотного слоя"
   ],
   "metadata": {
    "id": "zOUH2L_YTvK6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DenseEvalCode = '''\n",
    "#define _RELU(x) ( ((x) > 0.0f) ? (x) : 0.0f )\n",
    "#define _SIGMOID(x)  ( 1.0f / (1.0f + expf(-(x)) ))\n",
    "__global__ void dense_eval(int num_outputs, int num_inputs, int relu, int sigmoid, float * w, float * b, \\\n",
    "                           float * x, float *y, int batch_size, int w_t, int b_t, float delta)\n",
    "{\n",
    "     int i = blockDim.x*blockIdx.x + threadIdx.x;\n",
    "     if (i < num_outputs)\n",
    "     {\n",
    "         // Ваш код (применить формулу плотного слоя x - вход, у - выход, w - веса, b - базис)\n",
    "        if( w_t >= 0 && i == (w_t / num_inputs))\n",
    "        {\n",
    "              int j = w_t % num_inputs;\n",
    "              for(int k=0; k < batch_size; k++)\n",
    "                  y[k*num_outputs + i] += delta*x[k*num_inputs+j];\n",
    "        }\n",
    "\n",
    "        if( b_t >= 0 && i == b_t )\n",
    "        {\n",
    "              for(int k=0; k < batch_size; k++)\n",
    "                  y[k*num_outputs + i] += delta;\n",
    "        }\n",
    "        if(relu > 0 || sigmoid > 0)\n",
    "             for(int k=0; k < batch_size; k++)\n",
    "             {\n",
    "                  float temp = y[k * num_outputs + i];\n",
    "                  if (relu > 0)\n",
    "                      temp = _RELU(temp);\n",
    "                  if (sigmoid > 0)\n",
    "                      temp = _SIGMOID(temp);\n",
    "                  y[k * num_outputs + i] = temp;\n",
    "             }\n",
    "    }\n",
    "    return;\n",
    "}\n",
    "'''\n",
    "eval_mod = SourceModule(DenseEvalCode)\n",
    "eval_ker = eval_mod.get_function('dense_eval')"
   ],
   "metadata": {
    "id": "O4F-D5wXTNFl",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.296301Z",
     "start_time": "2024-12-02T16:50:57.291820Z"
    }
   },
   "outputs": [],
   "execution_count": 258
  },
  {
   "cell_type": "code",
   "source": [
    "class DenseLayer:\n",
    "\n",
    "    def __init__(self, num_inputs=None, num_outputs=None, weights=None, b=None, stream=None, \\\n",
    "    relu=False, sigmoid=False, delta=None):\n",
    "        self.stream = stream\n",
    "        if delta is None:\n",
    "            self.delta = np.float32(0.001)\n",
    "        else:\n",
    "            self.delta = np.float32(delta)\n",
    "        if weights is None:\n",
    "            weights = (np.random.rand(num_outputs, num_inputs) -.5 )\n",
    "            self.num_inputs = np.int32(num_inputs)\n",
    "            self.num_outputs = np.int32(num_outputs)\n",
    "\n",
    "        if type(weights) != pycuda.gpuarray.GPUArray:\n",
    "            self.weights = gpuarray.to_gpu_async(np.array(weights, dtype=np.float32) , stream = self.stream)\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        if num_inputs is None or num_outputs is None:\n",
    "            self.num_inputs = np.int32(self.weights.shape[1])\n",
    "            self.num_outputs = np.int32(self.weights.shape[0])\n",
    "\n",
    "        else:\n",
    "            self.num_inputs = np.int32(num_inputs)\n",
    "            self.num_outputs = np.int32(num_outputs)\n",
    "        if b is None:\n",
    "            b = gpuarray.zeros((self.num_outputs,),dtype=np.float32)\n",
    "        if type(b) != pycuda.gpuarray.GPUArray:\n",
    "            self.b = gpuarray.to_gpu_async(np.array(b, dtype=np.float32) , stream = self.stream)\n",
    "        else:\n",
    "            self.b = b\n",
    "        self.relu = np.int32(relu)\n",
    "        self.sigmoid = np.int32(sigmoid)\n",
    "        self.block = (32,1,1)\n",
    "        self.grid = (int(np.ceil(self.num_outputs / 32)), 1,1)\n",
    "\n",
    "    def eval_(self, x, y=None, batch_size=None, stream=None, delta=None, w_t = None, b_t = None):\n",
    "        if stream is None:\n",
    "            stream = self.stream\n",
    "        if type(x) != pycuda.gpuarray.GPUArray:\n",
    "            x = gpuarray.to_gpu_async(np.array(x,dtype=np.float32) , stream=self.stream)\n",
    "\n",
    "        if batch_size is None:\n",
    "            if len(x.shape) == 2:\n",
    "                batch_size = np.int32(x.shape[0])\n",
    "            else:\n",
    "                batch_size = np.int32(1)\n",
    "        if delta is None:\n",
    "            delta = self.delta\n",
    "        delta = np.float32(delta)\n",
    "        if w_t is None:\n",
    "            w_t = np.int32(-1)\n",
    "        if b_t is None:\n",
    "            b_t = np.int32(-1)\n",
    "        if y is None:\n",
    "            if batch_size == 1:\n",
    "                y = gpuarray.empty((self.num_outputs,), dtype=np.float32)\n",
    "            else:\n",
    "                y = gpuarray.empty((batch_size, self.num_outputs), dtype=np.float32)\n",
    "        eval_ker(self.num_outputs, self.num_inputs, self.relu, self.sigmoid, \\\n",
    "                 self.weights, self.b, x, y, np.int32(batch_size), w_t, b_t, \\\n",
    "                 delta , block=self.block, grid=self.grid , stream=stream)\n",
    "\n",
    "        return y"
   ],
   "metadata": {
    "id": "QtgHVOfwTSZg",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.322611Z",
     "start_time": "2024-12-02T16:50:57.314415Z"
    }
   },
   "outputs": [],
   "execution_count": 259
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Реализация Softmax слоя"
   ],
   "metadata": {
    "id": "2UMPVI7zUDYA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "SoftmaxExpCode='''\n",
    "__global__ void softmax_exp( int num, float *x, float *y, int batch_size)\n",
    "{\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i < num)\n",
    "    {\n",
    "        for (int k=0; k < batch_size; k++)\n",
    "        {\n",
    "            y[num*k + i] = expf(x[num*k+i]);\n",
    "\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "exp_mod = SourceModule(SoftmaxExpCode)\n",
    "exp_ker = exp_mod.get_function('softmax_exp')"
   ],
   "metadata": {
    "id": "m00hVSTnTUr6",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.344706Z",
     "start_time": "2024-12-02T16:50:57.339949Z"
    }
   },
   "outputs": [],
   "execution_count": 260
  },
  {
   "cell_type": "code",
   "source": [
    "SoftmaxMeanCode='''\n",
    "__global__ void softmax_mean( int num, float *x, float *y, int batch_size)\n",
    "{\n",
    "    int i = blockDim.x*blockIdx.x + threadIdx.x;\n",
    "    if (i < batch_size)\n",
    "    {\n",
    "        float temp = 0.0f;\n",
    "        for(int k=0; k < num; k++)\n",
    "            temp += x[i*num + k];\n",
    "        for(int k=0; k < num; k++)\n",
    "            y[i*num+k] = x[i*num+k] / temp;\n",
    "    }\n",
    "    return;\n",
    "}'''\n",
    "mean_mod = SourceModule(SoftmaxMeanCode)\n",
    "mean_ker = mean_mod.get_function('softmax_mean')"
   ],
   "metadata": {
    "id": "PpPQPnhgTVNY",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.366243Z",
     "start_time": "2024-12-02T16:50:57.362718Z"
    }
   },
   "outputs": [],
   "execution_count": 261
  },
  {
   "cell_type": "code",
   "source": [
    "class SoftmaxLayer:\n",
    "\n",
    "    def __init__(self, num=None, stream=None):\n",
    "        self.num = np.int32(num)\n",
    "        self.stream = stream\n",
    "\n",
    "    def eval_(self, x, y=None, batch_size=None, stream=None):\n",
    "        if stream is None:\n",
    "            stream = self.stream\n",
    "        if type(x) != pycuda.gpuarray.GPUArray:\n",
    "            temp = np.array(x,dtype=np.float32)\n",
    "            x = gpuarray.to_gpu_async( temp , stream=stream)\n",
    "        if batch_size==None:\n",
    "            if len(x.shape) == 2:\n",
    "                batch_size = np.int32(x.shape[0])\n",
    "            else:\n",
    "                batch_size = np.int32(1)\n",
    "        else:\n",
    "            batch_size = np.int32(batch_size)\n",
    "        if y is None:\n",
    "            if batch_size == 1:\n",
    "                y = gpuarray.empty((self.num,), dtype=np.float32)\n",
    "            else:\n",
    "                y = gpuarray.empty((batch_size, self.num), dtype=np.float32)\n",
    "        exp_ker(self.num, x, y, batch_size, block=(32,1,1), grid=(int( np.ceil( self.num / 32) ), 1, 1), stream=stream)\n",
    "        mean_ker(self.num, y, y, batch_size, block=(32,1,1), grid=(int( np.ceil( batch_size / 32)), 1,1), stream=stream)\n",
    "\n",
    "        return y"
   ],
   "metadata": {
    "id": "5V0pMm2jTXcO",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.389205Z",
     "start_time": "2024-12-02T16:50:57.384192Z"
    }
   },
   "outputs": [],
   "execution_count": 262
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Реализация нейронной сети"
   ],
   "metadata": {
    "id": "DNRkrANuUKv9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SequentialNetwork:\n",
    "\n",
    "    def __init__(self, layers=None, delta=None, stream = None, max_batch_size=32, max_streams=10, epochs = 10):\n",
    "        self.network = []\n",
    "        self.network_summary = []\n",
    "        self.network_mem = []\n",
    "        if stream is not None:\n",
    "            self.stream = stream\n",
    "        else:\n",
    "            self.stream = drv.Stream()\n",
    "        if delta is None:\n",
    "            delta = 0.0001\n",
    "        self.delta = delta\n",
    "        self.max_batch_size=max_batch_size\n",
    "        self.max_streams = max_streams\n",
    "        self.epochs = epochs\n",
    "        if layers is not None:\n",
    "            for layer in layers:\n",
    "                self.add_layer(self, layer)\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        if layer['type'] == 'dense':\n",
    "            if len(self.network) == 0:\n",
    "                num_inputs = layer['num_inputs']\n",
    "            else:\n",
    "                num_inputs = self.network_summary[-1][2]\n",
    "            num_outputs = layer['num_outputs']\n",
    "            sigmoid = layer['sigmoid']\n",
    "            relu = layer['relu']\n",
    "            weights = layer['weights']\n",
    "            b = layer['bias']\n",
    "            self.network.append(DenseLayer(num_inputs=num_inputs, num_outputs=num_outputs, sigmoid=sigmoid, relu=relu, weights=weights, b=b))\n",
    "            self.network_summary.append( ('dense', num_inputs, num_outputs))\n",
    "            if self.max_batch_size > 1:\n",
    "                if len(self.network_mem) == 0:\n",
    "                    self.network_mem.append(gpuarray.empty( (self.max_batch_size, self.network_summary[-1][1] ), dtype=np.float32 ) )\n",
    "                self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][2] ), dtype=np.float32  ) )\n",
    "            else:\n",
    "                if len(self.network_mem) == 0:\n",
    "                    self.network_mem.append( gpuarray.empty( (self.network_summary[-1][1], ), dtype=np.float32 ) )\n",
    "                self.network_mem.append( gpuarray.empty((self.network_summary[-1][2], ), dtype=np.float32  ) )\n",
    "        elif layer['type'] == 'softmax':\n",
    "            if len(self.network) == 0:\n",
    "                raise Exception(\"Error!  Softmax layer can't be first!\")\n",
    "            if self.network_summary[-1][0] != 'dense':\n",
    "                raise Exception(\"Error!  Need a dense layer before a softmax layer!\")\n",
    "            num = self.network_summary[-1][2]\n",
    "            self.network.append(SoftmaxLayer(num=num))\n",
    "            self.network_summary.append(('softmax', num, num))\n",
    "            if self.max_batch_size > 1:\n",
    "                self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][2] ), dtype=np.float32  ) )\n",
    "            else:\n",
    "                self.network_mem.append( gpuarray.empty((self.network_summary[-1][2], ), dtype=np.float32  ) )\n",
    "\n",
    "    def predict(self, x, stream=None):\n",
    "        if stream is None:\n",
    "            stream = self.stream\n",
    "        if type(x) != np.ndarray:\n",
    "            temp = np.array(x, dtype = np.float32)\n",
    "            x = temp\n",
    "        if(x.size == self.network_mem[0].size):\n",
    "            self.network_mem[0].set_async(x, stream=stream)\n",
    "        else:\n",
    "            if x.size > self.network_mem[0].size:\n",
    "                raise Exception(\"Error: batch size too large for input.\")\n",
    "            x0 = np.zeros((self.network_mem[0].size,), dtype=np.float32)\n",
    "            x0[0:x.size] = x.ravel()\n",
    "            self.network_mem[0].set_async(x0.reshape(self.network_mem[0].shape), stream=stream)\n",
    "        if(len(x.shape) == 2):\n",
    "            batch_size = x.shape[0]\n",
    "        else:\n",
    "            batch_size = 1\n",
    "        for i in np.arange(len(self.network)):\n",
    "            self.network[i].eval_(x=self.network_mem[i], y = self.network_mem[i+1], batch_size=batch_size, stream = stream)\n",
    "        y = self.network_mem[-1].get_async(stream=stream)\n",
    "        if len(y.shape) == 2:\n",
    "            y = y[0:batch_size, :]\n",
    "\n",
    "        return y\n",
    "\n",
    "    def partial_predict(self, layer_index=None, w_t=None, b_t=None, partial_mem=None, stream=None, batch_size=None, delta=None):\n",
    "        self.network[layer_index].eval_(x=self.network_mem[layer_index], y = partial_mem[layer_index+1], batch_size=batch_size, stream = stream, w_t=w_t, b_t=b_t, delta=delta)\n",
    "        for i in np.arange(layer_index+1, len(self.network)):\n",
    "            self.network[i].eval_(x=partial_mem[i], y =partial_mem[i+1], batch_size=batch_size, stream = stream)\n",
    "\n",
    "    def bsgd(self, training=None, labels=None, delta=None, max_streams = None, batch_size = None, epochs = 1, training_rate=0.01):\n",
    "        training_rate = np.float32(training_rate)\n",
    "        training = np.float32(training)\n",
    "        labels = np.float32(labels)\n",
    "        if( training.shape[0] != labels.shape[0] ):\n",
    "            raise Exception(\"Number of training data points should be same as labels!\")\n",
    "        if max_streams is None:\n",
    "            max_streams = self.max_streams\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "        if delta is None:\n",
    "            delta = self.delta\n",
    "        streams = []\n",
    "        bgd_mem = []\n",
    "        for _ in np.arange(max_streams):\n",
    "            streams.append(drv.Stream())\n",
    "            bgd_mem.append([])\n",
    "        for i in np.arange(len(bgd_mem)):\n",
    "            for mem_bank in self.network_mem:\n",
    "                bgd_mem[i].append( gpuarray.empty_like(mem_bank) )\n",
    "        num_points = training.shape[0]\n",
    "        if batch_size is None:\n",
    "            batch_size = self.max_batch_size\n",
    "        index = np.arange(training.shape[0])\n",
    "        for k in np.arange(epochs):\n",
    "            print('-----------------------------------------------------------')\n",
    "            print(f'Starting training epoch: {k}')\n",
    "            print(f'Batch size: {batch_size} , Total number of training samples: {num_points}')\n",
    "            print('-----------------------------------------------------------')\n",
    "            all_grad = []\n",
    "            np.random.shuffle(index)\n",
    "            for r in np.arange( int(np.floor(training.shape[0] / batch_size)) ):\n",
    "                batch_index = index[r*batch_size:(r+1)*batch_size]\n",
    "                batch_training = training[batch_index, :]\n",
    "                batch_labels = labels[batch_index, :]\n",
    "                batch_predictions = self.predict(batch_training)\n",
    "                cur_entropy = cross_entropy(predictions=batch_predictions, ground_truth=batch_labels)\n",
    "                print(f'entropy: {cur_entropy}')\n",
    "                for i in np.arange(len(self.network)):\n",
    "                    if self.network_summary[i][0] != 'dense':\n",
    "                        continue\n",
    "                    all_weights = Queue()\n",
    "                    grad_w = np.zeros((self.network[i].weights.size,), dtype=np.float32)\n",
    "                    grad_b = np.zeros((self.network[i].b.size,), dtype=np.float32)\n",
    "                    for w in np.arange( self.network[i].weights.size ):\n",
    "                        all_weights.put( ('w', np.int32(w) ) )\n",
    "                    for b in np.arange( self.network[i].b.size ):\n",
    "                        all_weights.put(('b', np.int32(b) ) )\n",
    "                    while not all_weights.empty():\n",
    "                        stream_weights = Queue()\n",
    "                        for j in np.arange(max_streams):\n",
    "                            if all_weights.empty():\n",
    "                                break\n",
    "                            wb = all_weights.get()\n",
    "                            if wb[0] == 'w':\n",
    "                                w_t = wb[1]\n",
    "                                b_t = None\n",
    "                            elif wb[0] == 'b':\n",
    "                                b_t = wb[1]\n",
    "                                w_t = None\n",
    "                            stream_weights.put( wb )\n",
    "                            self.partial_predict(layer_index=i, w_t=w_t, b_t=b_t, partial_mem=bgd_mem[j], stream=streams[j], batch_size=batch_size, delta=delta)\n",
    "                        for j in np.arange(max_streams):\n",
    "                            if stream_weights.empty():\n",
    "                                break\n",
    "                            wb = stream_weights.get()\n",
    "                            w_predictions = bgd_mem[j][-1].get_async(stream=streams[j])\n",
    "                            w_entropy = cross_entropy(predictions=w_predictions[:batch_size,:], ground_truth=batch_labels)\n",
    "                            if wb[0] == 'w':\n",
    "                                w_t = wb[1]\n",
    "                                grad_w[w_t] = -(w_entropy - cur_entropy) / delta\n",
    "                            elif wb[0] == 'b':\n",
    "                                b_t = wb[1]\n",
    "                                grad_b[b_t] = -(w_entropy - cur_entropy) / delta\n",
    "                    all_grad.append([np.reshape(grad_w,self.network[i].weights.shape) , grad_b])\n",
    "            for i in np.arange(len(self.network)):\n",
    "                if self.network_summary[i][0] == 'dense':\n",
    "                    new_weights = self.network[i].weights.get()\n",
    "                    new_weights += training_rate*all_grad[i][0]\n",
    "                    new_bias = self.network[i].b.get()\n",
    "                    new_bias += training_rate*all_grad[i][1]\n",
    "                    self.network[i].weights.set(new_weights)\n",
    "                    self.network[i].b.set(new_bias)"
   ],
   "metadata": {
    "id": "m7sOE1SKTY-c",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.424851Z",
     "start_time": "2024-12-02T16:50:57.407221Z"
    }
   },
   "outputs": [],
   "execution_count": 263
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T16:50:57.452089Z",
     "start_time": "2024-12-02T16:50:57.448234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize(data):\n",
    "    norm = np.linalg.norm(data)\n",
    "    norm_data = data / norm\n",
    "    \n",
    "    # norm_data[norm_data == 0] = 1e-12\n",
    "    return norm_data"
   ],
   "outputs": [],
   "execution_count": 264
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Main"
   ],
   "metadata": {
    "id": "fhAYpuONUUcs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "     to_class = { 'Bream' : [1,0,0] , 'Roach' : [0,1,0], 'Perch' : [0,0,1]}\n",
    "     fish_data = []\n",
    "     fish_labels = []\n",
    "     with open('fish.data', 'r') as csvfile:\n",
    "         csvreader = csv.reader(csvfile, delimiter=',')\n",
    "         for row in csvreader:\n",
    "             newrow = []\n",
    "             for i in range(len(row)-1):\n",
    "                 newrow.append(row[i])\n",
    "             fish_data.append(newrow)\n",
    "             fish_labels.append(to_class[row[len(row) - 1]])\n",
    "     \n",
    "     # Ваш код (Привести формат массива типам данных для GPU)\n",
    "     fish_data = np.float32(fish_data)\n",
    "     fish_labels = np.float32(fish_labels)\n",
    "     \n",
    "     # Ваш код (Разделить на тестовую и тренировочную выборку)\n",
    "     \n",
    "     iris_len = len(fish_data)\n",
    "     shuffled_index = list(range(iris_len))\n",
    "     np.random.shuffle(shuffled_index)\n",
    "     fish_data = fish_data[shuffled_index, :]\n",
    "     fish_labels = fish_labels[shuffled_index, :]\n",
    "     fish_train = fish_data[:iris_len // 3 * 2]\n",
    "     fish_test = fish_data[iris_len // 3 * 2 + 1:]\n",
    "     label_train = fish_labels[:iris_len // 3 * 2]\n",
    "     label_test = fish_labels[iris_len // 3 * 2 + 1:]\n",
    "\n",
    "     \n",
    "     sn = SequentialNetwork( max_batch_size=32 )\n",
    "     # Ваш код (Добавить слои нейронной сети, на выходе нужен слой softmax)\n",
    "     sn.add_layer({'type' : 'dense', 'num_inputs' : 6, 'num_outputs' : 10, 'relu': True, 'sigmoid': False, 'weights' : None, 'bias' : None} )\n",
    "     sn.add_layer({'type' : 'dense', 'num_inputs' : 10, 'num_outputs' : 20, 'relu': True, 'sigmoid': False, 'weights': None, 'bias' : None} )\n",
    "     sn.add_layer({'type' : 'dense', 'num_inputs' : 20, 'num_outputs' : 3, 'relu': False, 'sigmoid': True, 'weights': None, 'bias' : None} )\n",
    "     sn.add_layer({'type' : 'softmax'})\n",
    "\n",
    "     # Ваш код (Провести нормализацию данных)\n",
    "     fish_train = normalize(fish_train)\n",
    "     fish_test = normalize(fish_test)\n",
    "     time_start = time()\n",
    "     sn.bsgd(training=fish_train, labels=label_train, batch_size=16,\n",
    "             max_streams=20, epochs=20, delta=0.0001, training_rate=1)\n",
    "     time_end = time()\n",
    "     hits = 0\n",
    "     for i in range(fish_test.shape[0]):\n",
    "         if np.argmax(sn.predict(fish_test[i, :])) == np.argmax(label_test[i, :]):\n",
    "             hits += 1\n",
    "     print(f'Percentage Correct Classifications: {round((float(hits) / fish_test.shape[0] * 100), 2)} %')\n",
    "     print(f'Total Training Time: {np.round((time_end - time_start),2)}')\n"
   ],
   "metadata": {
    "id": "SKCa4U_ZTFE_",
    "ExecuteTime": {
     "end_time": "2024-12-02T16:51:26.532549Z",
     "start_time": "2024-12-02T16:50:57.472327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Starting training epoch: 0\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 1\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 2\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 3\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 4\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 5\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 6\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 7\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 8\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 9\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 10\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 11\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 12\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 13\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 14\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 15\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 16\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 17\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 18\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 19\n",
      "Batch size: 16 , Total number of training samples: 74\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "entropy: 0.6036433855372281\n",
      "Percentage Correct Classifications: 33.33 %\n",
      "Total Training Time: 29.04\n"
     ]
    }
   ],
   "execution_count": 265
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T16:51:26.556090Z",
     "start_time": "2024-12-02T16:51:26.553108Z"
    }
   },
   "cell_type": "code",
   "source": "print(fish_test.shape[0])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "execution_count": 266
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T16:51:26.583042Z",
     "start_time": "2024-12-02T16:51:26.575640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(fish_train)\n",
    "print(fish_test)\n",
    "print(fish_data)\n",
    "print(shuffled_index)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00106983 0.08960371 0.00552209 0.00604304 0.00708494 0.00259309]\n",
      " [0.00106362 0.09898084 0.00591801 0.0064598  0.00754338 0.00297209]\n",
      " [0.00076424 0.02604759 0.00395923 0.004376   0.00468857 0.00118621]\n",
      " [0.00096593 0.06251422 0.00560544 0.00598053 0.00627226 0.00158061]\n",
      " [0.00063281 0.01771236 0.00370918 0.00408426 0.00433432 0.00107058]\n",
      " [0.00097731 0.08126848 0.00575131 0.00625142 0.00729333 0.00264018]\n",
      " [0.00105707 0.07084945 0.00614723 0.00666818 0.0077726  0.00289918]\n",
      " [0.00130542 0.14586651 0.00718914 0.00771009 0.0082102  0.00225781]\n",
      " [0.00086003 0.04167615 0.00460521 0.00489695 0.0055846  0.00154135]\n",
      " [0.00058799 0.01437827 0.00343828 0.00379253 0.00423013 0.00110406]\n",
      " [0.00087607 0.05209518 0.00539706 0.00583466 0.00612639 0.00162962]\n",
      " [0.00140601 0.20317121 0.00779344 0.00854361 0.00956468 0.00388326]\n",
      " [0.00154546 0.22921881 0.00835607 0.00896037 0.00948132 0.00260736]\n",
      " [0.00069216 0.02083807 0.00337577 0.00375085 0.00400091 0.00108825]\n",
      " [0.00081394 0.03750853 0.00491779 0.00525119 0.00581382 0.00147671]\n",
      " [0.00138156 0.1708722  0.00773092 0.00833523 0.00885618 0.00232032]\n",
      " [0.00083248 0.02292188 0.00416761 0.00458438 0.00489695 0.00115078]\n",
      " [0.00041659 0.00666818 0.00260476 0.00285482 0.0030632  0.00073517]\n",
      " [0.00073454 0.02500569 0.00416761 0.00458438 0.00489695 0.00117527]\n",
      " [0.00086353 0.045427   0.00520952 0.00552209 0.00583466 0.00149367]\n",
      " [0.00073454 0.0281314  0.00416761 0.00458438 0.00489695 0.00122424]\n",
      " [0.00071006 0.02500569 0.00416761 0.00458438 0.00489695 0.00127321]\n",
      " [0.00125091 0.13544747 0.0076059  0.00812685 0.00862696 0.00232065]\n",
      " [0.0007758  0.04688567 0.00458438 0.00500114 0.00531371 0.00151972]\n",
      " [0.00047261 0.00833523 0.00268811 0.00293817 0.00337577 0.0008642 ]\n",
      " [0.00150555 0.18754266 0.00771009 0.00833523 0.00885618 0.00244431]\n",
      " [0.00143124 0.22921881 0.00812685 0.00875199 0.00929378 0.00266732]\n",
      " [0.00110027 0.12919605 0.00656399 0.00718914 0.00827271 0.00323463]\n",
      " [0.00090333 0.05417899 0.00529287 0.00573047 0.0060222  0.00149351]\n",
      " [0.00075517 0.03125711 0.0042718  0.00468857 0.00500114 0.00141532]\n",
      " [0.00076757 0.03750853 0.00479276 0.00520952 0.00552209 0.00134187]\n",
      " [0.00079706 0.03021521 0.00458438 0.00500114 0.00531371 0.00132843]\n",
      " [0.00151643 0.20838073 0.00829355 0.00896037 0.00941881 0.00248657]\n",
      " [0.00068647 0.02500569 0.00404259 0.004376   0.00493862 0.00127416]\n",
      " [0.00126904 0.15003413 0.00666818 0.00729333 0.00846026 0.00340948]\n",
      " [0.00129163 0.17712362 0.00683489 0.00750171 0.00866864 0.00351947]\n",
      " [0.00054837 0.01073161 0.00312571 0.00337577 0.00358415 0.00095697]\n",
      " [0.00069287 0.0291733  0.004376   0.00468857 0.00520952 0.00136489]\n",
      " [0.00075559 0.03125711 0.004376   0.00479276 0.00510533 0.00108743]\n",
      " [0.00086807 0.03917558 0.0047094  0.00512617 0.00545958 0.00140311]\n",
      " [0.00116464 0.14586651 0.00664735 0.00729333 0.00843942 0.00338421]\n",
      " [0.00079918 0.03521634 0.00458438 0.00500114 0.00566796 0.00157002]\n",
      " [0.00106978 0.12711224 0.00643896 0.00698075 0.0080435  0.00325762]\n",
      " [0.00119352 0.13544747 0.0064598  0.00698075 0.00806433 0.00301606]\n",
      " [0.00148084 0.17712362 0.00768925 0.00833523 0.0088145  0.00248569]\n",
      " [0.00132668 0.1437827  0.00720997 0.00771009 0.00818936 0.00220294]\n",
      " [0.00088333 0.041051   0.00489695 0.00533455 0.00562628 0.00136719]\n",
      " [0.00077622 0.03542472 0.00448019 0.00489695 0.00520952 0.00130759]\n",
      " [0.00061295 0.01458665 0.00327158 0.00362582 0.00385504 0.00095605]\n",
      " [0.00143033 0.1427408  0.00708494 0.0076059  0.00812685 0.00226739]\n",
      " [0.00074479 0.02500569 0.00387588 0.00416761 0.00462605 0.00129529]\n",
      " [0.00169664 0.20838073 0.00837691 0.00906456 0.00958551 0.00262643]\n",
      " [0.00133103 0.11585969 0.00666818 0.00718914 0.0076059  0.00213726]\n",
      " [0.00111909 0.1427408  0.00654315 0.00708494 0.00816853 0.00333276]\n",
      " [0.00073642 0.0270895  0.00402175 0.00443851 0.00475108 0.0013303 ]\n",
      " [0.00099473 0.05667956 0.00520952 0.00562628 0.00637645 0.00178541]\n",
      " [0.00060522 0.0162537  0.00364666 0.00391756 0.00441767 0.00116185]\n",
      " [0.00073454 0.0270895  0.00416761 0.00458438 0.00489695 0.00127321]\n",
      " [0.00121934 0.14878385 0.00681405 0.00750171 0.0086478  0.00344182]\n",
      " [0.0012775  0.1416989  0.00662651 0.00729333 0.00846026 0.00322336]\n",
      " [0.00158282 0.20838073 0.00856445 0.00916875 0.00971054 0.00260243]\n",
      " [0.0002934  0.00122945 0.00156286 0.0017504  0.00183375 0.0004401 ]\n",
      " [0.00131113 0.19900359 0.00729333 0.00802266 0.00916875 0.00376836]\n",
      " [0.00066153 0.01812912 0.00379253 0.00412594 0.00462605 0.00117039]\n",
      " [0.00131411 0.19171028 0.00729333 0.00802266 0.00918959 0.00375854]\n",
      " [0.00099431 0.06668183 0.00579298 0.00625142 0.00658483 0.00158694]\n",
      " [0.00137594 0.20838073 0.00698075 0.00771009 0.00887702 0.00395027]\n",
      " [0.00100935 0.09377133 0.00575131 0.00625142 0.00731416 0.00291835]\n",
      " [0.00100327 0.10419036 0.00598053 0.0064598  0.00754338 0.00299472]\n",
      " [0.00155502 0.21150644 0.00771009 0.00833523 0.00883534 0.00257992]\n",
      " [0.00091021 0.10419036 0.00606388 0.00656399 0.00758506 0.00286715]\n",
      " [0.00140651 0.19275218 0.00754338 0.00823104 0.00943965 0.00390801]\n",
      " [0.00116276 0.12502843 0.00612639 0.00666818 0.00775176 0.00321698]\n",
      " [0.00116058 0.11981892 0.00652232 0.00708494 0.00823104 0.00315249]]\n",
      "[[0.00267463 0.29833755 0.01331604 0.01418923 0.01502603 0.00452283]\n",
      " [0.00123544 0.04002089 0.00694908 0.00756759 0.00840439 0.00224397]\n",
      " [0.00194829 0.14189225 0.01073288 0.01153329 0.01273392 0.00345089]\n",
      " [0.00179272 0.16372183 0.00975054 0.01080564 0.01262477 0.00494891]\n",
      " [0.00100853 0.03092523 0.00662164 0.00727653 0.00764035 0.00184897]\n",
      " [0.00283628 0.30561408 0.01182435 0.01273392 0.01357072 0.00417978]\n",
      " [0.00231776 0.34563497 0.0138254  0.01491688 0.01691792 0.00641189]\n",
      " [0.00188127 0.21829577 0.01069649 0.01164244 0.01353434 0.0054408 ]\n",
      " [0.00192297 0.2546784  0.01106032 0.01200627 0.01393455 0.0054066 ]\n",
      " [0.00170856 0.12370094 0.00869545 0.0096414  0.011315   0.00450337]\n",
      " [0.00156649 0.10550962 0.00873183 0.00956863 0.01135138 0.00454055]\n",
      " [0.00189099 0.2546784  0.01106032 0.01200627 0.01400731 0.00543484]\n",
      " [0.00146258 0.08804596 0.00844077 0.00924119 0.01091479 0.00419128]\n",
      " [0.00186908 0.10914788 0.00916842 0.00993246 0.01044181 0.00302813]\n",
      " [0.00088483 0.01455305 0.0050208  0.00545739 0.00582122 0.00139127]\n",
      " [0.00138993 0.05821221 0.00745844 0.00818609 0.0092048  0.00255894]\n",
      " [0.00220231 0.26377407 0.01156968 0.01273392 0.0148805  0.0059522 ]\n",
      " [0.00110974 0.0291061  0.00625781 0.0069127  0.00734929 0.00205045]\n",
      " [0.00163605 0.10550962 0.00873183 0.00945948 0.01062373 0.00322961]\n",
      " [0.0016613  0.09095657 0.00924119 0.01000522 0.01051458 0.00264967]\n",
      " [0.0027263  0.32744366 0.01327966 0.01418923 0.01506241 0.00405179]\n",
      " [0.00132113 0.05857603 0.00800418 0.00851354 0.00971416 0.00251597]\n",
      " [0.00162103 0.13206895 0.00956863 0.01055096 0.01218818 0.00463151]\n",
      " [0.0012934  0.04002089 0.0069127  0.00764035 0.00818609 0.00207108]\n",
      " [0.00218951 0.18700671 0.0110967  0.0119335  0.01237009 0.00364918]\n",
      " [0.00245248 0.2546784  0.01237009 0.01309775 0.01393455 0.00385987]\n",
      " [0.00113638 0.02837845 0.00611228 0.00680355 0.00705823 0.00189161]\n",
      " [0.00136595 0.05457394 0.00742206 0.00800418 0.00898651 0.00211183]\n",
      " [0.00180436 0.18191314 0.01036905 0.01116947 0.01317051 0.00517601]\n",
      " [0.00138254 0.05821221 0.00767673 0.00818609 0.00909566 0.00232849]\n",
      " [0.00132069 0.05275481 0.0075312  0.00825886 0.0088046  0.00216593]\n",
      " [0.00192046 0.18191314 0.00975054 0.01080564 0.01255201 0.00515887]\n",
      " [0.0012194  0.         0.0069127  0.00745844 0.00829524 0.00235585]\n",
      " [0.00129078 0.05275481 0.00745844 0.00800418 0.00884098 0.00241359]\n",
      " [0.00120336 0.04184002 0.0069127  0.00764035 0.00818609 0.00215294]\n",
      " [0.00157719 0.09641396 0.00924119 0.01000522 0.01051458 0.00256556]]\n",
      "[[   5.134   430.       26.5      29.       34.       12.444 ]\n",
      " [   5.1042  475.       28.4      31.       36.2      14.2628]\n",
      " [   3.6675  125.       19.       21.       22.5       5.6925]\n",
      " [   4.6354  300.       26.9      28.7      30.1       7.5852]\n",
      " [   3.0368   85.       17.8      19.6      20.8       5.1376]\n",
      " [   4.69    390.       27.6      30.       35.       12.67  ]\n",
      " [   5.0728  340.       29.5      32.       37.3      13.9129]\n",
      " [   6.2646  700.       34.5      37.       39.4      10.835 ]\n",
      " [   4.1272  200.       22.1      23.5      26.8       7.3968]\n",
      " [   2.8217   69.       16.5      18.2      20.3       5.2983]\n",
      " [   4.2042  250.       25.9      28.       29.4       7.8204]\n",
      " [   6.7473  975.       37.4      41.       45.9      18.6354]\n",
      " [   7.4165 1100.       40.1      43.       45.5      12.5125]\n",
      " [   3.3216  100.       16.2      18.       19.2       5.2224]\n",
      " [   3.906   180.       23.6      25.2      27.9       7.0866]\n",
      " [   6.63    820.       37.1      40.       42.5      11.135 ]\n",
      " [   3.995   110.       20.       22.       23.5       5.5225]\n",
      " [   1.9992   32.       12.5      13.7      14.7       3.528 ]\n",
      " [   3.525   120.       20.       22.       23.5       5.64  ]\n",
      " [   4.144   218.       25.       26.5      28.        7.168 ]\n",
      " [   3.525   135.       20.       22.       23.5       5.875 ]\n",
      " [   3.4075  120.       20.       22.       23.5       6.11  ]\n",
      " [   6.003   650.       36.5      39.       41.4      11.1366]\n",
      " [   3.723   225.       22.       24.       25.5       7.293 ]\n",
      " [   2.268    40.       12.9      14.1      16.2       4.1472]\n",
      " [   7.225   900.       37.       40.       42.5      11.73  ]\n",
      " [   6.8684 1100.       39.       42.       44.6      12.8002]\n",
      " [   5.2801  620.       31.5      34.5      39.7      15.5227]\n",
      " [   4.335   260.       25.4      27.5      28.9       7.1672]\n",
      " [   3.624   150.       20.5      22.5      24.        6.792 ]\n",
      " [   3.6835  180.       23.       25.       26.5       6.4395]\n",
      " [   3.825   145.       22.       24.       25.5       6.375 ]\n",
      " [   7.2772 1000.       39.8      43.       45.2      11.9328]\n",
      " [   3.2943  120.       19.4      21.       23.7       6.1146]\n",
      " [   6.09    720.       32.       35.       40.6      16.3618]\n",
      " [   6.1984  850.       32.8      36.       41.6      16.8896]\n",
      " [   2.6316   51.5      15.       16.2      17.2       4.5924]\n",
      " [   3.325   140.       21.       22.5      25.        6.55  ]\n",
      " [   3.626   150.       21.       23.       24.5       5.2185]\n",
      " [   4.1658  188.       22.6      24.6      26.2       6.7334]\n",
      " [   5.589   700.       31.9      35.       40.5      16.2405]\n",
      " [   3.8352  169.       22.       24.       27.2       7.5344]\n",
      " [   5.1338  610.       30.9      33.5      38.6      15.633 ]\n",
      " [   5.7276  650.       31.       33.5      38.7      14.4738]\n",
      " [   7.1064  850.       36.9      40.       42.3      11.9286]\n",
      " [   6.3666  690.       34.6      37.       39.3      10.5717]\n",
      " [   4.239   197.       23.5      25.6      27.        6.561 ]\n",
      " [   3.725   170.       21.5      23.5      25.        6.275 ]\n",
      " [   2.9415   70.       15.7      17.4      18.5       4.588 ]\n",
      " [   6.864   685.       34.       36.5      39.       10.881 ]\n",
      " [   3.5742  120.       18.6      20.       22.2       6.216 ]\n",
      " [   8.142  1000.       40.2      43.5      46.       12.604 ]\n",
      " [   6.3875  556.       32.       34.5      36.5      10.2565]\n",
      " [   5.3704  685.       31.4      34.       39.2      15.9936]\n",
      " [   3.534   130.       19.3      21.3      22.8       6.384 ]\n",
      " [   4.7736  272.       25.       27.       30.6       8.568 ]\n",
      " [   2.9044   78.       17.5      18.8      21.2       5.5756]\n",
      " [   3.525   130.       20.       22.       23.5       6.11  ]\n",
      " [   5.8515  714.       32.7      36.       41.5      16.517 ]\n",
      " [   6.1306  680.       31.8      35.       40.6      15.4686]\n",
      " [   7.5958 1000.       41.1      44.       46.6      12.4888]\n",
      " [   1.408     5.9       7.5       8.4       8.8       2.112 ]\n",
      " [   6.292   955.       35.       38.5      44.       18.084 ]\n",
      " [   3.1746   87.       18.2      19.8      22.2       5.6166]\n",
      " [   6.3063  920.       35.       38.5      44.1      18.0369]\n",
      " [   4.7716  320.       27.8      30.       31.6       7.6156]\n",
      " [   6.603  1000.       33.5      37.       42.6      18.957 ]\n",
      " [   4.8438  450.       27.6      30.       35.1      14.0049]\n",
      " [   4.8146  500.       28.7      31.       36.2      14.3714]\n",
      " [   7.4624 1015.       37.       40.       42.4      12.3808]\n",
      " [   4.368   500.       29.1      31.5      36.4      13.7592]\n",
      " [   6.7497  925.       36.2      39.5      45.3      18.7542]\n",
      " [   5.58    600.       29.4      32.       37.2      15.438 ]\n",
      " [   5.5695  575.       31.3      34.       39.5      15.1285]\n",
      " [   3.624   130.       20.5      22.5      24.        5.856 ]\n",
      " [   7.3514  820.       36.6      39.       41.3      12.4313]\n",
      " [   3.3957  110.       19.1      20.8      23.1       6.1677]\n",
      " [   5.355   390.       29.5      31.7      35.        9.485 ]\n",
      " [   4.9274  450.       26.8      29.7      34.7      13.6024]\n",
      " [   2.772    85.       18.2      20.       21.        5.082 ]\n",
      " [   7.7957  840.       32.5      35.       37.3      11.4884]\n",
      " [   6.3705  950.       38.       41.       46.5      17.6235]\n",
      " [   5.1708  600.       29.4      32.       37.2      14.9544]\n",
      " [   5.2854  700.       30.4      33.       38.3      14.8604]\n",
      " [   4.6961  340.       23.9      26.5      31.1      12.3778]\n",
      " [   4.3056  290.       24.       26.3      31.2      12.48  ]\n",
      " [   5.1975  700.       30.4      33.       38.5      14.938 ]\n",
      " [   4.02    242.       23.2      25.4      30.       11.52  ]\n",
      " [   5.1373  300.       25.2      27.3      28.7       8.323 ]\n",
      " [   2.432    40.       13.8      15.       16.        3.824 ]\n",
      " [   3.8203  160.       20.5      22.5      25.3       7.0334]\n",
      " [   6.0532  725.       31.8      35.       40.9      16.36  ]\n",
      " [   3.0502   80.       17.2      19.       20.2       5.6358]\n",
      " [   4.4968  290.       24.       26.       29.2       8.8768]\n",
      " [   4.5662  250.       25.4      27.5      28.9       7.2828]\n",
      " [   7.4934  900.       36.5      39.       41.4      11.1366]\n",
      " [   3.6312  161.       22.       23.4      26.7       6.9153]\n",
      " [   4.4555  363.       26.3      29.       33.5      12.73  ]\n",
      " [   3.555   110.       19.       21.       22.5       5.6925]\n",
      " [   6.018   514.       30.5      32.8      34.       10.03  ]\n",
      " [   6.7408  700.       34.       36.       38.3      10.6091]\n",
      " [   3.1234   78.       16.8      18.7      19.4       5.1992]\n",
      " [   3.7544  150.       20.4      22.       24.7       5.8045]\n",
      " [   4.9594  500.       28.5      30.7      36.2      14.2266]\n",
      " [   3.8     160.       21.1      22.5      25.        6.4   ]\n",
      " [   3.63    145.       20.7      22.7      24.2       5.9532]\n",
      " [   5.2785  500.       26.8      29.7      34.5      14.1795]\n",
      " [   3.3516    0.       19.       20.5      22.8       6.4752]\n",
      " [   3.5478  145.       20.5      22.       24.3       6.6339]\n",
      " [   3.3075  115.       19.       21.       22.5       5.9175]\n",
      " [   4.335   265.       25.4      27.5      28.9       7.0516]]\n",
      "[4, 10, 67, 90, 63, 7, 13, 97, 50, 36, 89, 33, 108, 60, 51, 105, 73, 56, 70, 84, 72, 69, 100, 79, 35, 103, 106, 22, 86, 75, 82, 80, 107, 42, 26, 28, 58, 46, 77, 81, 24, 48, 18, 19, 102, 98, 83, 78, 59, 95, 39, 109, 93, 21, 68, 53, 37, 71, 27, 23, 110, 55, 31, 38, 30, 91, 29, 8, 11, 104, 12, 32, 15, 20, 74, 101, 41, 54, 5, 64, 94, 34, 14, 16, 2, 1, 17, 0, 85, 57, 45, 25, 62, 52, 88, 99, 49, 3, 65, 92, 96, 61, 43, 9, 47, 76, 6, 40, 44, 66, 87]\n"
     ]
    }
   ],
   "execution_count": 267
  }
 ]
}
